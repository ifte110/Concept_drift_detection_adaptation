{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport shap\nfrom scipy.stats import mannwhitneyu, ttest_ind, ks_2samp, ttest_ind_from_stats\nimport copy\nimport random\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader, random_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-02T11:28:17.391215Z","iopub.execute_input":"2024-10-02T11:28:17.391563Z","iopub.status.idle":"2024-10-02T11:28:22.970368Z","shell.execute_reply.started":"2024-10-02T11:28:17.391533Z","shell.execute_reply":"2024-10-02T11:28:22.969437Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Set seed for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:28:33.839521Z","iopub.execute_input":"2024-10-02T11:28:33.840031Z","iopub.status.idle":"2024-10-02T11:28:33.906760Z","shell.execute_reply.started":"2024-10-02T11:28:33.840004Z","shell.execute_reply":"2024-10-02T11:28:33.905934Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Define the model architecture\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n        x = x.view(-1, 64 * 5 * 5)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return torch.log_softmax(x, dim=1)\n\n# Function to simulate client training with MNIST data\ndef client_training(train_loader, model, criterion, optimizer, num_epochs=1):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss, correct, total = 0.0, 0.0, 0.0\n        for images, labels in train_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        accuracy = correct / total\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss}, Accuracy: {accuracy}\")\n        \n    return total_loss\n    \n\n# Function to test the trained model on client data\ndef client_testing(test_loader, model):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:28:36.546636Z","iopub.execute_input":"2024-10-02T11:28:36.547307Z","iopub.status.idle":"2024-10-02T11:28:36.562547Z","shell.execute_reply.started":"2024-10-02T11:28:36.547273Z","shell.execute_reply":"2024-10-02T11:28:36.561476Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Function to compute SHAP values\ndef compute_shap_values(model, shap_data, background):\n#     reference_tensor = shap_data[0]\n#     p_values = []\n\n#     for tensor in shap_data:\n#         if tensor.shape != reference_tensor.shape:\n#             tensor = np.reshape(tensor, reference_tensor.shape)\n#         t_stat, p_value = ttest_ind(tensor.flatten().cpu().numpy(), reference_tensor.flatten().cpu().numpy())\n#         p_values.append(p_value)\n\n#     sorted_indices = np.argsort(p_values)\n#     num_keep = 100\n#     indices_to_keep = sorted_indices[-num_keep:]\n#     flt_tensor_list = [shap_data[index] for index in indices_to_keep]\n\n#     data = torch.stack(flt_tensor_list).to(device)\n    explainer = shap.DeepExplainer(model, background)\n    x, labels = explainer.shap_values(shap_data, ranked_outputs=1, check_additivity=False)\n    \n    x_shap_value = np.array(x[0])\n    shap_values_reshaped = x_shap_value.reshape(x_shap_value.shape[0], -1)\n    features = [f'Feature_{i}' for i in range(shap_values_reshaped.shape[1])]\n    shap_mean_values = np.mean(shap_values_reshaped, axis=0)\n    \n    valid_indices = np.sum(np.abs(shap_values_reshaped), axis=0) >= 0.5\n    filtered_shap_values = shap_mean_values[valid_indices]\n    filtered_features = [feature for i, feature in enumerate(features) if valid_indices[i]]\n    \n    return filtered_shap_values, filtered_features","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:33:24.648187Z","iopub.execute_input":"2024-10-02T11:33:24.649051Z","iopub.status.idle":"2024-10-02T11:33:24.660525Z","shell.execute_reply.started":"2024-10-02T11:33:24.649009Z","shell.execute_reply":"2024-10-02T11:33:24.659242Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"b = [2, 6, 9, 11, 15, 22, 31, 44 , 51, 57]\n\na = len(b)\n\na","metadata":{"execution":{"iopub.status.busy":"2024-09-09T17:28:03.755344Z","iopub.execute_input":"2024-09-09T17:28:03.755700Z","iopub.status.idle":"2024-09-09T17:28:03.763256Z","shell.execute_reply.started":"2024-09-09T17:28:03.755670Z","shell.execute_reply":"2024-09-09T17:28:03.762344Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"code","source":"def federated_learning(num_clients=5, num_local_epochs=5, num_global_epochs=5):\n    \n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n    \n    partition_size = len(trainset) // num_clients\n    lengths = [partition_size] * num_clients\n    datasets = random_split(trainset, lengths)\n    \n    client_indices = [dataset.indices for dataset in datasets]\n    \n    # clients that will experience concept drift\n    drift_clients = [2, 6, 9, 11, 15, 22, 31, 44 , 51, 57]\n\n    # Initialize global model\n    global_model = CNN().to(device)\n    previous_shap_values_per_client = [None] * num_clients\n    previous_shap_features_per_client = [None] * num_clients\n    drastic_change_factor = 3  # Example threshold for drastic change\n    previous_client_losses = [0.0] * num_clients\n    drastic_change_detected = [False] * num_clients\n    sustained_high_loss = [False] * num_clients\n    \n    # Initialize client-wise feature occurrence dictionary\n    feature_occurrence_dict = {client_id: {} for client_id in range(num_clients)}\n    frequent_features_per_client = {client_id: [] for client_id in range(num_clients)}\n\n    # Initialize counters for drift detection performance\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    # Federated learning process\n    for global_epoch in range(num_global_epochs):\n        print(f\"Global Epoch {global_epoch + 1}/{num_global_epochs}\")\n\n        # Swap classes if the current global epoch is more than half\n        if global_epoch == 10:\n            print(\"Swapping classes 0 and 1 in the train dataset for each client\")\n            for client_id in drift_clients:\n                indices = client_indices[client_id]\n                for idx in indices:\n                    if trainset.targets[idx] == 8:\n                        trainset.targets[idx] = 3\n                    elif trainset.targets[idx] == 3:\n                        trainset.targets[idx] = 8\n                        \n                    # Swap 5 and 6\n                    elif trainset.targets[idx] == 6:\n                        trainset.targets[idx] = 5\n                    elif trainset.targets[idx] == 5:\n                        trainset.targets[idx] = 6\n                    \n        client_train_loaders = [torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, indices), batch_size=32, shuffle=True) for indices in client_indices]\n        client_test_loaders = [torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False) for _ in range(num_clients)]\n        client_models = [copy.deepcopy(global_model) for _ in range(num_clients)]\n\n        # Perform local training and compute SHAP values\n        shap_values_per_client = []\n        shap_features_per_client = []\n        client_losses = []\n        \n        for i, train_loader in enumerate(client_train_loaders):\n            print(f\"\\tTraining Client {i + 1}/{num_clients}\")\n            criterion = nn.CrossEntropyLoss()\n            optimizer = optim.Adam(client_models[i].parameters(), lr=0.001)\n            loss = client_training(train_loader, client_models[i], criterion, optimizer, num_local_epochs)\n            client_losses.append(loss)\n\n            client_shap_data = []\n            client_shap_labels = []\n            client_class_count = {i: 0 for i in range(10)}\n\n            for batch_idx, (images, labels) in enumerate(train_loader):\n                for img, lbl in zip(images, labels):\n                    if client_class_count[lbl.item()] <= 10:  # Accumulate up to num images per class\n                        client_shap_data.append(img)\n                        client_shap_labels.append(lbl)\n                        client_class_count[lbl.item()] += 1\n\n                if all(count >= 10 for count in client_class_count.values()):\n                    break\n            \n            for label in range(10):\n                if client_class_count[label] < 10:\n                    continue\n                shap_data = torch.stack(client_shap_data).to(device)\n                shap_labels = torch.stack(client_shap_labels).to(device)\n\n            # Split shap_data into 90% and 10% sets\n            num_samples = shap_data.size(0)\n            split_idx = int(num_samples * 0.9)\n            background = shap_data[split_idx:].to(device)  # 10% of data for background\n            shap_data = shap_data[:split_idx].to(device)  # 90% of data for SHAP computation\n            shap_labels = shap_labels[:split_idx].to(device)  # Corresponding labels\n\n            # Compute SHAP values\n            print(f\"\\t\\tComputed SHAP values for Client {i + 1}/{num_clients}\")\n            shap_values, shap_features = compute_shap_values(client_models[i], shap_data, background)\n            shap_values_per_client.append(shap_values)\n            shap_features_per_client.append(shap_features)\n\n        # Perform Kolmogorov-Smirnov test on SHAP values\n        print(\"\\tPerforming Kolmogorov–Smirnov test for SHAP values\")\n        p_values = []\n        for i, (shap_values_client, shap_features_client) in enumerate(zip(shap_values_per_client, shap_features_per_client)):\n            if previous_shap_values_per_client[i] is not None:\n                if global_epoch >= 5 and frequent_features_per_client[i]:\n                    common_features = list(set(shap_features_client) & set(previous_shap_features_per_client[i]) & set(frequent_features_per_client[i]))\n                else:\n                    common_features = list(set(shap_features_client) & set(previous_shap_features_per_client[i]))\n                if common_features:\n                    current_indices = [shap_features_client.index(feature) for feature in common_features]\n                    previous_indices = [previous_shap_features_per_client[i].index(feature) for feature in common_features]\n\n                    current_shap_values = shap_values_client[current_indices]\n                    previous_shap_values = previous_shap_values_per_client[i][previous_indices]\n\n                    u_statistic, p_value = ks_2samp(previous_shap_values, current_shap_values)\n                    p_values.append(p_value)\n                    print(f\"Global Epoch {global_epoch + 1}, Client {i+1}: Kolmogorov–Smirnov test statistic: {u_statistic}, p-value: {p_value}\")\n\n            # Update previous SHAP values\n            previous_shap_values_per_client[i] = shap_values_client\n            previous_shap_features_per_client[i] = shap_features_client\n            \n        # Update client-wise feature occurrence dictionary\n        for client_id, shap_features_client in enumerate(shap_features_per_client):\n            for feature in shap_features_client:\n                if feature in feature_occurrence_dict[client_id]:\n                    feature_occurrence_dict[client_id][feature] += 1\n                else:\n                    feature_occurrence_dict[client_id][feature] = 1\n                    \n                \n        # Concept drift detection and performance metrics\n        if global_epoch >= 5:\n            # Update frequent features list for each client\n            for client_id in range(num_clients):\n                frequent_features_per_client[client_id] = [\n                    feature for feature, count in feature_occurrence_dict[client_id].items() if count >= 10\n                ]\n\n            # Drift detection from SHAP values\n            detected_drift_clients_1 = [i for i, p_value in enumerate(p_values) if p_value < 0.005]\n\n            # Drift detection using loss values\n            for i, (prev_loss, curr_loss) in enumerate(zip(previous_client_losses, client_losses)):\n                if not drastic_change_detected[i]:\n                    if curr_loss > (prev_loss * drastic_change_factor):\n                        drastic_change_detected[i] = True\n                else:\n                    if curr_loss >= 4:\n                        sustained_high_loss[i] = True\n                    else:\n                        drastic_change_detected[i] = False\n                        sustained_high_loss[i] = False\n\n            detected_drift_clients_2 = [\n                i for i, (drastic, sustained) in enumerate(zip(drastic_change_detected, sustained_high_loss))\n                if drastic and sustained\n            ]\n            \n            # List of clients from both drift detection logics\n            detected_drift_clients = list(set(detected_drift_clients_1) | set(detected_drift_clients_2))\n\n            for client_id in range(num_clients):\n                ground_truth_drift = (client_id in drift_clients) and (global_epoch >= 10)\n                detected_drift = client_id in detected_drift_clients_1\n\n                if detected_drift and ground_truth_drift:\n                    true_positives += 1\n                elif not detected_drift and not ground_truth_drift:\n                    true_negatives += 1\n                elif detected_drift and not ground_truth_drift:\n                    false_positives += 1\n                elif not detected_drift and ground_truth_drift:\n                    false_negatives += 1\n\n                if detected_drift == ground_truth_drift:\n                    print(f\"Global Epoch {global_epoch + 1}, Client {client_id + 1}: Drift detection is accurate.\")\n                else:\n                    print(f\"Global Epoch {global_epoch + 1}, Client {client_id + 1}: Drift detection is not accurate.\")\n                    \n                    \n        # Update loss tracking\n        previous_client_losses = client_losses.copy()\n                \n        # Aggregate client models into global model\n        print(\"\\tAggregating client models\")\n        global_model = aggregate_models(global_model, client_models)\n            \n            \n    # Test models on each client\n    accuracies = []\n    for i, test_loader in enumerate(client_test_loaders):\n        print(f\"Testing Client {i + 1}/{num_clients}\")\n        accuracy = client_testing(test_loader, global_model)\n        accuracies.append(accuracies)\n        print(f\"\\tAccuracy for Client {i + 1}/{num_clients}: {accuracy}\")\n        \n    # Calculate performance metrics of drift detection\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n\n    print(f\"Drift Detection metircs::: true_positives:{true_positives},true_negatives:{true_negatives},false_positives:{false_positives},false_negatives:{false_negatives}\")\n    print(f\"Drift Detection Performance:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1_score}\")\n\n# Aggregation of models' weights using FedAvg\ndef aggregate_models(global_model, client_models):\n    global_dict = global_model.state_dict()\n    for k in global_dict.keys():\n        global_dict[k] = torch.stack([client_model.state_dict()[k].float() for client_model in client_models], 0).mean(0)\n    global_model.load_state_dict(global_dict)\n    return global_model\n    \n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Example usage\nif __name__ == \"__main__\":\n    client_accuracies=federated_learning(num_clients=60, num_local_epochs=1, num_global_epochs=40)\n    print(\"Client Accuracies:\", client_accuracies)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-02T11:33:27.212629Z","iopub.execute_input":"2024-10-02T11:33:27.213011Z","iopub.status.idle":"2024-10-02T11:33:38.879398Z","shell.execute_reply.started":"2024-10-02T11:33:27.212981Z","shell.execute_reply":"2024-10-02T11:33:38.878060Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Global Epoch 1/40\n\tTraining Client 1/60\nEpoch 1/1, Loss: 51.99495720863342, Accuracy: 0.477\n\t\tComputed SHAP values for Client 1/60\n1\n(99, 1, 28, 28)\n\tTraining Client 2/60\nEpoch 1/1, Loss: 53.000810980796814, Accuracy: 0.484\n\t\tComputed SHAP values for Client 2/60\n1\n(99, 1, 28, 28)\n\tTraining Client 3/60\nEpoch 1/1, Loss: 53.53186875581741, Accuracy: 0.485\n\t\tComputed SHAP values for Client 3/60\n1\n(98, 1, 28, 28)\n\tTraining Client 4/60\nEpoch 1/1, Loss: 53.37810117006302, Accuracy: 0.508\n\t\tComputed SHAP values for Client 4/60\n1\n(98, 1, 28, 28)\n\tTraining Client 5/60\nEpoch 1/1, Loss: 54.2434920668602, Accuracy: 0.485\n\t\tComputed SHAP values for Client 5/60\n1\n(99, 1, 28, 28)\n\tTraining Client 6/60\nEpoch 1/1, Loss: 51.45190727710724, Accuracy: 0.521\n\t\tComputed SHAP values for Client 6/60\n1\n(99, 1, 28, 28)\n\tTraining Client 7/60\nEpoch 1/1, Loss: 51.16041475534439, Accuracy: 0.506\n\t\tComputed SHAP values for Client 7/60\n1\n(99, 1, 28, 28)\n\tTraining Client 8/60\nEpoch 1/1, Loss: 50.179173827171326, Accuracy: 0.528\n\t\tComputed SHAP values for Client 8/60\n1\n(98, 1, 28, 28)\n\tTraining Client 9/60\nEpoch 1/1, Loss: 52.082065999507904, Accuracy: 0.499\n\t\tComputed SHAP values for Client 9/60\n1\n(98, 1, 28, 28)\n\tTraining Client 10/60\nEpoch 1/1, Loss: 50.054236352443695, Accuracy: 0.509\n\t\tComputed SHAP values for Client 10/60\n1\n(99, 1, 28, 28)\n\tTraining Client 11/60\nEpoch 1/1, Loss: 54.23165434598923, Accuracy: 0.469\n\t\tComputed SHAP values for Client 11/60\n1\n(98, 1, 28, 28)\n\tTraining Client 12/60\nEpoch 1/1, Loss: 54.74217492341995, Accuracy: 0.454\n\t\tComputed SHAP values for Client 12/60\n1\n(99, 1, 28, 28)\n\tTraining Client 13/60\nEpoch 1/1, Loss: 52.4888841509819, Accuracy: 0.521\n\t\tComputed SHAP values for Client 13/60\n1\n(98, 1, 28, 28)\n\tTraining Client 14/60\nEpoch 1/1, Loss: 52.24657869338989, Accuracy: 0.484\n\t\tComputed SHAP values for Client 14/60\n1\n(99, 1, 28, 28)\n\tTraining Client 15/60\nEpoch 1/1, Loss: 50.98832309246063, Accuracy: 0.52\n\t\tComputed SHAP values for Client 15/60\n1\n(99, 1, 28, 28)\n\tTraining Client 16/60\nEpoch 1/1, Loss: 49.3622375279665, Accuracy: 0.535\n\t\tComputed SHAP values for Client 16/60\n1\n(99, 1, 28, 28)\n\tTraining Client 17/60\nEpoch 1/1, Loss: 50.47368401288986, Accuracy: 0.539\n\t\tComputed SHAP values for Client 17/60\n1\n(99, 1, 28, 28)\n\tTraining Client 18/60\nEpoch 1/1, Loss: 55.016857743263245, Accuracy: 0.503\n\t\tComputed SHAP values for Client 18/60\n1\n(99, 1, 28, 28)\n\tTraining Client 19/60\nEpoch 1/1, Loss: 48.10744035243988, Accuracy: 0.559\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 227\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 227\u001b[0m     client_accuracies\u001b[38;5;241m=\u001b[39m\u001b[43mfederated_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_local_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_global_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient Accuracies:\u001b[39m\u001b[38;5;124m\"\u001b[39m, client_accuracies)\n","Cell \u001b[0;32mIn[11], line 76\u001b[0m, in \u001b[0;36mfederated_learning\u001b[0;34m(num_clients, num_local_epochs, num_global_epochs)\u001b[0m\n\u001b[1;32m     73\u001b[0m client_shap_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m client_class_count \u001b[38;5;241m=\u001b[39m {i: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)}\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img, lbl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, labels):\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m client_class_count[lbl\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:  \u001b[38;5;66;03m# Accumulate up to num images per class\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Federated learning process with historical shap values\ndef federated_learning(num_clients=5, num_local_epochs=5, num_global_epochs=5):\n    \n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n    \n#     # Restrict the training dataset to only 10,000 images\n#     num_train_images = 10000\n#     trainset.data = trainset.data[:num_train_images]\n#     trainset.targets = trainset.targets[:num_train_images]\n\n    partition_size = len(trainset) // num_clients\n    lengths = [partition_size] * num_clients\n    datasets = random_split(trainset, lengths)\n    \n    client_indices = [dataset.indices for dataset in datasets]\n    \n    # clients that will experience concept drift\n    drift_clients = [2,6,9]\n\n    # Initialize global model\n    global_model = CNN().to(device)\n    previous_shap_values_per_client = [None] * num_clients\n    previous_shap_features_per_client = [None] * num_clients\n    drastic_change_factor = 3  # Example threshold for drastic change\n    previous_client_losses = [0.0] * num_clients\n    drastic_change_detected = [False] * num_clients\n    sustained_high_loss = [False] * num_clients\n    \n    # Initialize feature occurrence dictionary\n    feature_occurrence_dict = {}\n    frequent_features = []\n\n\n    \n    # Initialize counters for drift detection performance\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    # Initialize historical SHAP values list\n    historical_shap_values_per_client = [[] for _ in range(num_clients)]\n\n\n    # Federated learning process\n    for global_epoch in range(num_global_epochs):\n        print(f\"Global Epoch {global_epoch + 1}/{num_global_epochs}\")\n\n        # Swap classes if the current global epoch is more than half\n        if global_epoch == 10: # (num_global_epochs // 2):\n            print(\"Swapping classes 0 and 1 in the train dataset for each client\")\n            for client_id in drift_clients:\n                indices = client_indices[client_id]\n                for idx in indices:\n                    if trainset.targets[idx] == 8:\n                        trainset.targets[idx] = 3\n                    elif trainset.targets[idx] == 3:\n                        trainset.targets[idx] = 8\n                    \n                    \n        client_train_loaders = [torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, indices), batch_size=32, shuffle=True) for indices in client_indices]\n        client_test_loaders = [torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False) for _ in range(num_clients)]\n        client_models = [copy.deepcopy(global_model) for _ in range(num_clients)]\n\n\n\n        # Perform local training and compute SHAP values\n        shap_values_per_client = []\n        shap_features_per_client = []\n        client_losses = []\n        \n        for i, train_loader in enumerate(client_train_loaders):\n            print(f\"\\tTraining Client {i + 1}/{num_clients}\")\n            criterion = nn.CrossEntropyLoss()\n            optimizer = optim.Adam(client_models[i].parameters(), lr=0.001)\n            loss = client_training(train_loader, client_models[i], criterion, optimizer, num_local_epochs)\n            client_losses.append(loss)   \n\n            client_shap_data = []\n            client_shap_labels = []\n            client_class_count = {i: 0 for i in range(10)}\n\n            for batch_idx, (images, labels) in enumerate(train_loader):\n                for img, lbl in zip(images, labels):\n                    if client_class_count[lbl.item()] < 10:  # Accumulate up to num images per class\n                        client_shap_data.append(img)\n                        client_shap_labels.append(lbl)\n                        client_class_count[lbl.item()] += 1\n\n                # Check if we've accumulated 10 images for each class\n                if all(count >= 10 for count in client_class_count.values()):\n                    break\n            \n            if all(count >= 10 for count in client_class_count.values()):\n                shap_data = torch.stack(client_shap_data).to(device)\n                shap_labels = torch.stack(client_shap_labels).to(device)\n\n                # Split shap_data into 90% and 10% sets\n                num_samples = shap_data.size(0)\n                split_idx = int(num_samples * 0.9)\n                background = shap_data[split_idx:].to(device)  # 10% of data for background\n                shap_data = shap_data[:split_idx].to(device)  # 90% of data for SHAP computation\n                shap_labels = shap_labels[:split_idx].to(device)  # Corresponding labels\n\n                # Compute SHAP values by class\n                print(f\"\\t\\tComputed SHAP values for Client {i + 1}/{num_clients}\")\n                shap_values, shap_features = compute_shap_values(client_models[i], shap_data, background)\n                shap_values_per_client.append(shap_values)\n                shap_features_per_client.append(shap_features)\n                \n                # Save current SHAP values to historical record\n                historical_shap_values_per_client[i].append(shap_values)\n                \n\n\n        # Perform Kolmogorov-Smirnov test on SHAP values\n        print(\"\\tPerforming Kolmogorov–Smirnov test for SHAP values\")\n        p_values = []\n        for i, (shap_values_client, shap_features_client) in enumerate(zip(shap_values_per_client, shap_features_per_client)):\n            if len(historical_shap_values_per_client[i]) > 1:\n                # Align and pad historical SHAP values if necessary\n                max_length = max(len(shap) for shap in historical_shap_values_per_client[i])\n                aligned_historical_shap_values = pad_shap_values(historical_shap_values_per_client[i], max_length)\n\n                # Calculate mean of aligned historical SHAP values\n                historical_shap_values_mean = np.max(aligned_historical_shap_values, axis=0)\n\n                # Align and pad current SHAP values if necessary\n                if len(shap_values_client.shape) == 1:  # If shap_values_client is 1-dimensional\n                    shap_values_client = shap_values_client.reshape(-1, 1)\n                current_shap_values = pad_shap_values([shap_values_client], max_length)[0]\n\n                # Perform KS test using mean historical SHAP values and current SHAP values\n                ks_stat, p_value = ks_2samp(historical_shap_values_mean.flatten(), current_shap_values.flatten())\n                p_values.append(p_value)\n                print(f\"Global Epoch {global_epoch + 1}, Client {i+1}: Kolmogorov–Smirnov test statistic: {ks_stat}, p-value: {p_value}\")\n            # Update previous SHAP values\n#             previous_shap_values_per_client[i] = shap_values_client\n#             previous_shap_features_per_client[i] = shap_features_client\n            \n        # Update feature occurrence dictionary\n        for shap_features_client in shap_features_per_client:\n            for feature in shap_features_client:\n                if feature in feature_occurrence_dict:\n                    feature_occurrence_dict[feature] += 1\n                else:\n                    feature_occurrence_dict[feature] = 1\n                \n\n        # concept drift detection and performance metrics\n        if global_epoch >= 5:\n            \n            #frequest features list\n            frequent_features = [feature for feature, count in feature_occurrence_dict.items() if count >= 20]\n            \n            # drift detected from shap values\n            detected_drift_clients_1 = [i for i, p_value in enumerate(p_values) if p_value < 0.005]\n            \n            # drift detection using loss values\n            for i, (prev_loss, curr_loss) in enumerate(zip(previous_client_losses, client_losses)):\n                if not drastic_change_detected[i]:\n                    if curr_loss > (prev_loss * drastic_change_factor):\n                        drastic_change_detected[i] = True\n                else:\n                    if curr_loss >= 4:\n                        sustained_high_loss[i] = True\n                    else:\n                        drastic_change_detected[i] = False\n                        sustained_high_loss[i] = False\n\n                \n                \n            detected_drift_clients_2 = [\n                i for i, (drastic, sustained) in enumerate(zip(drastic_change_detected, sustained_high_loss))\n                if drastic and sustained\n            ]\n            \n            # list of clients from both drift detection logics\n            detected_drift_clients = list(set(detected_drift_clients_1) | set(detected_drift_clients_2))\n            \n\n            for client_id in range(num_clients):\n                ground_truth_drift = (client_id in drift_clients) and (global_epoch>=11)\n                detected_drift = client_id in detected_drift_clients\n\n                if detected_drift and ground_truth_drift:\n                    true_positives += 1\n                elif not detected_drift and not ground_truth_drift:\n                    true_negatives += 1\n                elif detected_drift and not ground_truth_drift:\n                    false_positives += 1\n                elif not detected_drift and ground_truth_drift:\n                    false_negatives += 1\n\n                if detected_drift == ground_truth_drift:\n                    print(f\"Global Epoch {global_epoch + 1}, Client {client_id + 1}: Drift detection is accurate.\")\n                else:\n                    print(f\"Global Epoch {global_epoch + 1}, Client {client_id + 1}: Drift detection is inaccurate.\")\n                    \n        # Update loss tracking\n        previous_client_losses = client_losses.copy()\n\n                    \n        # Aggregate client models into global model\n        print(\"\\tAggregating client models\")\n        global_model = aggregate_models(global_model, client_models)\n\n    # Test models on each client\n    accuracies = []\n    for i, test_loader in enumerate(client_test_loaders):\n        print(f\"Testing Client {i + 1}/{num_clients}\")\n        accuracy = client_testing(test_loader, global_model)\n        accuracies.append(accuracy)\n        print(f\"\\tAccuracy for Client {i + 1}/{num_clients}: {accuracy}\")\n        \n    # Calculate performance metrics of drift detection\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n\n    print(f\"Drift Detection metircs::: true_positives:{true_positives},true_negatives:{true_negatives},false_positives:{false_positives},false_negatives:{false_negatives}\")\n    print(f\"Drift Detection Performance:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1_score}\")\n        \n\n    return accuracies\n\n# Aggregation of models' weights using FedAvg\ndef aggregate_models(global_model, client_models):\n    global_dict = global_model.state_dict()\n    for k in global_dict.keys():\n        global_dict[k] = torch.stack([client_model.state_dict()[k].float() for client_model in client_models], 0).mean(0)\n    global_model.load_state_dict(global_dict)\n    return global_model\n\n# Function to visualize images with specific classes\ndef visualize_images(images, labels, classes_to_visualize=[0, 1], num_images=10):\n    fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n    count = 0\n    for img, lbl in zip(images, labels):\n        if lbl.item() in classes_to_visualize:\n            axes[count].imshow(img.cpu().numpy().squeeze(), cmap='gray')\n            axes[count].set_title(f\"Class {lbl.item()}\")\n            axes[count].axis('off')\n            count += 1\n        if count >= num_images:\n            break\n    plt.show()\n    \n    \n# Function to pad and align SHAP values to a common shape\ndef pad_shap_values(shap_values_list, max_length):\n    padded_shap_values = []\n    for shap_values in shap_values_list:\n        if len(shap_values.shape) == 1:  # If shap_values is 1-dimensional\n            shap_values = shap_values.reshape(-1, 1)\n        if len(shap_values) < max_length:\n            # Pad with zeros to the right\n            padding = np.zeros((max_length - len(shap_values), shap_values.shape[1]))\n            padded_shap_values.append(np.vstack((shap_values, padding)))\n        else:\n            padded_shap_values.append(shap_values)\n    return np.array(padded_shap_values)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Example usage\nif __name__ == \"__main__\":\n    client_accuracies=federated_learning(num_clients=10, num_local_epochs=1, num_global_epochs=20)\n    print(\"Client Accuracies:\", client_accuracies)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#p-value\ndef federated_learning(num_clients=5, num_local_epochs=5, num_global_epochs=5):\n    \n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n    partition_size = len(trainset) // num_clients\n    lengths = [partition_size] * num_clients\n    datasets = random_split(trainset, lengths)\n    \n    client_indices = [dataset.indices for dataset in datasets]\n    \n    # clients that will experience concept drift\n    drift_clients = [2,6,9]\n\n    # Initialize global model\n    global_model = CNN().to(device)\n    previous_shap_values_per_client = [None] * num_clients\n    previous_shap_features_per_client = [None] * num_clients\n    drastic_change_factor = 3  # Example threshold for drastic change\n    previous_client_losses = [0.0] * num_clients\n    drastic_change_detected = [False] * num_clients\n    sustained_high_loss = [False] * num_clients\n    \n    # Initialize feature occurrence dictionary\n    feature_occurrence_dict = {}\n    frequent_features = []\n\n    # Initialize p-value tracking\n    p_values_per_client = [[] for _ in range(num_clients)]\n    \n    # Initialize counters for drift detection performance\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    # Federated learning process\n    for global_epoch in range(num_global_epochs):\n        print(f\"Global Epoch {global_epoch + 1}/{num_global_epochs}\")\n\n        # Swap classes if the current global epoch is more than half\n        if global_epoch == 10: # (num_global_epochs // 2):\n            print(\"Swapping classes 0 and 1 in the train dataset for each client\")\n            for client_id in drift_clients:\n                indices = client_indices[client_id]\n                for idx in indices:\n                    if trainset.targets[idx] == 8:\n                        trainset.targets[idx] = 3\n                    elif trainset.targets[idx] == 3:\n                        trainset.targets[idx] = 8\n                    \n                    \n        client_train_loaders = [torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, indices), batch_size=20, shuffle=True) for indices in client_indices]\n        client_test_loaders = [torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False) for _ in range(num_clients)]\n        client_models = [copy.deepcopy(global_model) for _ in range(num_clients)]\n\n        # Perform local training and compute SHAP values\n        shap_values_per_client = []\n        shap_features_per_client = []\n        client_losses = []\n        \n        for i, train_loader in enumerate(client_train_loaders):\n            print(f\"\\tTraining Client {i + 1}/{num_clients}\")\n            criterion = nn.CrossEntropyLoss()\n            optimizer = optim.Adam(client_models[i].parameters(), lr=0.001)\n            loss = client_training(train_loader, client_models[i], criterion, optimizer, num_local_epochs)\n            client_losses.append(loss)   \n\n            client_shap_data = []\n            client_shap_labels = []\n            client_class_count = {i: 0 for i in range(10)}\n\n            for batch_idx, (images, labels) in enumerate(train_loader):\n                for img, lbl in zip(images, labels):\n                    if client_class_count[lbl.item()] < 10:  # Accumulate up to num images per class\n                        client_shap_data.append(img)\n                        client_shap_labels.append(lbl)\n                        client_class_count[lbl.item()] += 1\n\n                # Check if we've accumulated 10 images for each class\n                if all(count >= 10 for count in client_class_count.values()):\n                    break\n            \n            if all(count >= 10 for count in client_class_count.values()):\n                shap_data = torch.stack(client_shap_data).to(device)\n                shap_labels = torch.stack(client_shap_labels).to(device)\n\n                # Split shap_data into 90% and 10% sets\n                num_samples = shap_data.size(0)\n                split_idx = int(num_samples * 0.9)\n                background = shap_data[split_idx:].to(device)  # 10% of data for background\n                shap_data = shap_data[:split_idx].to(device)  # 90% of data for SHAP computation\n                shap_labels = shap_labels[:split_idx].to(device)  # Corresponding labels\n\n                # Compute SHAP values by class\n                print(f\"\\t\\tComputed SHAP values for Client {i + 1}/{num_clients}\")\n                shap_values, shap_features = compute_shap_values(client_models[i], shap_data, background)\n                shap_values_per_client.append(shap_values)\n                shap_features_per_client.append(shap_features)\n\n        # Perform Kolmogorov-Smirnov test on SHAP values\n        print(\"\\tPerforming Kolmogorov–Smirnov test for SHAP values\")\n        p_values = []\n        for i, (shap_values_client, shap_features_client) in enumerate(zip(shap_values_per_client, shap_features_per_client)):\n            if previous_shap_values_per_client[i] is not None:\n                if global_epoch >= 5 and frequent_features:\n                    common_features = list(set(shap_features_client) & set(previous_shap_features_per_client[i]) & set(frequent_features))\n                else:\n                    common_features = list(set(shap_features_client) & set(previous_shap_features_per_client[i]))\n                if common_features:\n                    current_indices = [shap_features_client.index(feature) for feature in common_features]\n                    previous_indices = [previous_shap_features_per_client[i].index(feature) for feature in common_features]\n\n                    current_shap_values = shap_values_client[current_indices]\n                    previous_shap_values = previous_shap_values_per_client[i][previous_indices]\n\n                    u_statistic, p_value = ks_2samp(previous_shap_values, current_shap_values)\n                    p_values.append(p_value)\n                    print(f\"Global Epoch {global_epoch + 1}, Client {i+1}: Kolmogorov–Smirnov test statistic: {u_statistic}, p-value: {p_value}\")\n                    \n                    # Store the current p-value\n                    p_values_per_client[i].append(p_value)\n\n            # Update previous SHAP values\n            previous_shap_values_per_client[i] = shap_values_client\n            previous_shap_features_per_client[i] = shap_features_client\n            \n        # Update feature occurrence dictionary\n        for shap_features_client in shap_features_per_client:\n            for feature in shap_features_client:\n                if feature in feature_occurrence_dict:\n                    feature_occurrence_dict[feature] += 1\n                else:\n                    feature_occurrence_dict[feature] = 1\n\n        # concept drift detection and performance metrics\n        if global_epoch >= 5:\n            \n            # frequent features list\n            frequent_features = [feature for feature, count in feature_occurrence_dict.items() if count >= 20]\n            \n            # drift detected from shap values\n            detected_drift_clients_1 = []\n            for i, p_value in enumerate(p_values):\n                if len(p_values_per_client[i]) > 1:\n                    avg_p_value = sum(p_values_per_client[i][:-1]) / len(p_values_per_client[i][:-1])\n                    if p_value < (avg_p_value)**2:\n                        detected_drift_clients_1.append(i)\n\n            # drift detection using loss values\n            for i, (prev_loss, curr_loss) in enumerate(zip(previous_client_losses, client_losses)):\n                if not drastic_change_detected[i]:\n                    if curr_loss > (prev_loss * drastic_change_factor):\n                        drastic_change_detected[i] = True\n                else:\n                    if curr_loss >= 4:\n                        sustained_high_loss[i] = True\n                    else:\n                        drastic_change_detected[i] = False\n                        sustained_high_loss[i] = False\n\n            detected_drift_clients_2 = [\n                i for i, (drastic, sustained) in enumerate(zip(drastic_change_detected, sustained_high_loss))\n                if drastic and sustained\n            ]\n            \n            # list of clients from both drift detection logics\n            detected_drift_clients = list(set(detected_drift_clients_1) | set(detected_drift_clients_2))\n\n            for client_id in range(num_clients):\n                ground_truth_drift = (client_id in drift_clients) and (global_epoch >= 10)\n                detected_drift = client_id in detected_drift_clients_1\n\n                if detected_drift and ground_truth_drift:\n                    true_positives += 1\n                elif not detected_drift and not ground_truth_drift:\n                    true_negatives += 1\n                elif detected_drift and not ground_truth_drift:\n                    false_positives += 1\n                elif not detected_drift and ground_truth_drift:\n                    false_negatives += 1\n\n                if detected_drift == ground_truth_drift:\n                    print(f\"Global Epoch {global_epoch + 1}, Client {client_id + 1}: Drift detection is accurate.\")\n                else:\n                    print(f\"Global Epoch {global_epoch + 1}, Client {client_id + 1}: Drift detection is NOT accurate.\")\n\n        previous_client_losses = client_losses\n        \n        print(\"\\tAggregating client models\")\n        global_model = aggregate_models(global_model, client_models)\n\n    # Test models on each client\n    accuracies = []\n    for i, test_loader in enumerate(client_test_loaders):\n        print(f\"Testing Client {i + 1}/{num_clients}\")\n        accuracy = client_testing(test_loader, global_model)\n        accuracies.append(accuracy)\n        print(f\"\\tAccuracy for Client {i + 1}/{num_clients}: {accuracy}\")\n\n    # Print final drift detection performance\n    print(\"\\nFinal Drift Detection Performance Metrics:\")\n    print(f\"True Positives: {true_positives}\")\n    print(f\"True Negatives: {true_negatives}\")\n    print(f\"False Positives: {false_positives}\")\n    print(f\"False Negatives: {false_negatives}\")\n\n# Aggregation of models' weights using FedAvg\ndef aggregate_models(global_model, client_models):\n    global_dict = global_model.state_dict()\n    for k in global_dict.keys():\n        global_dict[k] = torch.stack([client_model.state_dict()[k].float() for client_model in client_models], 0).mean(0)\n    global_model.load_state_dict(global_dict)\n    return global_model\n\n    \n\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Example usage\nif __name__ == \"__main__\":\n    client_accuracies=federated_learning(num_clients=10, num_local_epochs=1, num_global_epochs=20)\n    print(\"Client Accuracies:\", client_accuracies)\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}